---
title: "La Vuelta"
author: "Laura Cebollero Ruiz, Alexandre Rodr√≠guez Garau"
date: "4th January, 2019"
output:
  pdf_document:
    number_sections: yes
---

# Introduction
In this project we intend to predict the duration of the different stages of the cycling race *La Vuelta* using the information provided in the file $\texttt{Vuelta0.mtp}$. 


```{r}
library("readxl")
library("knitr")
library("ggplot2")
library("reshape2")
library("MASS")
library("DataExplorer")
```

We have exported the data to an extension that R can read: `.csv`.

```{r}
data<- read.csv("Vuelta01.csv", sep = ';', header = TRUE, dec=",")
```


# Data exploration

To predict the length of the stages we will use a set that contains 14 explanatory variables and a response variable called __`ForecastedTime`__. Let's take a look at the summary of the variables:

```{r}
kable(summary(data[,1:6]))
kable(summary(data[,6:11]))
kable(summary(data[,12:16]))
```

In the previous tables we can see a short summary of the variables. __`ports1`__, __`ports2`__ and __`ports3`__ indicate the number of mountain sections in a given stage and their category (1,2 or 3). __`year`__ corresponds to the year in which this data was recorded (1 to 6). __`Week`__ is the week of the race in wich the stage takes place (1 to 3). The variables __`bef_mount`__ and __`aft_mount`__ tell us whether a stage took place before or after a mountain stage, respectively. Similarly, the variables __`bef_tt`__ and __`aft_tt`__ indicate if a stage took place before or after a time trial stage. Finally, the variable __`last`__ tells us if that stage was the last of the whole race. 

By the looks of the summary we can't seem to find any outliers or abnormal values. Most of the explanatory variables range from 0 to very low values and are natural numbers. The only continuous variables are __`ForecastedTime`__, __`Distance`__, __`HeightInc`__ and __`AccumIncr`__. This makes sense because the first variable indicates time and time is a continuous variable and the rest indicate distance which can also be continous. 

The variable __`Distance`__ seems to be quite balanced with a mean of 193 and min and max values of 111 and 264 respectively. __`HeightIncr`__, however, is the only variable that presents negative values and has very high variance. Its minimum value is -940 and the maximum value is 2310. 

By taking a closer look at the data we detect some abnormal values in the __`last`__ variable: Since this variable indicates if a stage was the last of the whole race, then there should as many stages with this value equal to 1 as years of data have been recorded. Since the recorded stages are from 6 different years then there should be 6 rows, but instead there are only 5, meaning that there is at least 1 missing stage. 

```{r}
data[data$last == 1,]$year
```

If we look at the data we can easily see that the missing value belongs to the year 3. However, this will probably not greatly affect our predicting power. There is something to be said about the variables __`year`__, __`week`__, __`bef_mount`__, __`aft_mount`__, __`bef_tt`__, __`aft_tt`__ and __`last`__: all of these variables are categorical and indicate the group or category a row belongs to. For this, we should transform these variables into factors.

```{r}
data$year      <- as.factor(data$year)
data$week      <- as.factor(data$week)
data$bef_mount <- as.factor(data$bef_mount)
data$aft_mount <- as.factor(data$aft_mount)
data$aft_tt    <- as.factor(data$aft_tt)
data$bef_tt    <- as.factor(data$bef_tt)
data$last      <- as.factor(data$last) 
```
\newpage
Now that we have transformed the categorical columns to factors we should take another look at the summary of these variables:

```{r}
kable(summary(data[,9:15]))
```

We can se that for the variables __`year`__ and __`week`__ the variables are quite balanced, each group contains a very similar amount of observations. The rest, however, are more unbalanced. `bef_mount` has more than twice 0s than 1s, which means there are twice the stages that do not precede a mountain stage. The same happens with `aft_mount`, which makes sense for it means there are only 31 stages after a mountain stage. Since they are the same numbers as the `bef_mount`, this tells us that the mountain stages are always in the middle and are not the first or last stages.

In `bef_tt` there are 9 times more stages **not** preceding a time trial stage than does preceding it. There are 4 times less after trial stages.  And finally, as mentioned before, there are only 5 last stages and 100 that are normal ones.

Let's finally take a closer look to the numerical attributes:
```{r}
num <- 6:8; num <- append(num, 16)
kable(summary(data[,1:5]))
kable(summary(data[,num]))
```

We can see that the mean and median in Time are pretty close, which means that there is not an outlier that drastically decreases or increases the mean. The same happens with Distance, AccumIncr and ForacastedTime. 

However, HeightIncr has a low Median but a high mean, which means that in general there is not a stable increase of height but some stages have a high increase of it. And we can see that there is at least one stage that consists of a decrease of almost 1km, whereas there is at least one stage of an increase of over 2km.

As for the ports, although it is a numerical variable, it actually uses integer values and natural ones should not allowed.
We contemplated using the number of ports as a categorical variable. For example, in portsE we can see there are between 0 and 2 mountain passes:
```{r}
table(data$portsE)
```

However, by enforcing said restriction changing the variable as a categorical one, we would not be allowing new categorical variables like using 5 mountain passes in stage port3. Thus, meaning we would be restricted to a maximum of 3 in ports1 and ports2, 2 mountain passes on portsE, and a maximum of 4 mountain passes in ports3, which would not be correct, for maybe in the future they may be a case where there are 3 mountain passes in portsE, where there is still no case as seen in the table above.


# Data exploration visualization

Now, in order to find outliers and to see how the data behaves we will plot some boxplots. First of all we would like to see if the forecasted times are different depending on the year in which the race took place. 

```{r}
ggplot(data = data) +
  aes(x = year, y = ForecastedTime) +
  geom_boxplot(fill = "#4292c6") +
  theme_minimal()
```

At first glance we see that the medians are really similar among all years, excepting year 3 and year 6, which are about 25 units higher. The interquartile ranges for every year are different. Even though the Q3 percentile is very similar for all years, the Q1 percentile varies from 225 to 300. It is important to say that there appears to be an outlier in the second year.

Let's take a look at real Times:
```{r}
ggplot(data = data) +
  aes(x = year, y = Time) +
  geom_boxplot(fill = "#4292c6") +
  theme_minimal()
```
 We can see how the highest Time is on the 3rd year, and there's a lot of variance on the
 1st and 5th years' races.
 The second year seems to have some stages with a very high time, so there's a lot of variance on the upper bound,
while on the 5th year there does is a lot of variance on the lower bound, meaning there are some stages with less Time than the rest. 
The 5th year is the only one where this phenomena is so visible.

 And let's compare the forecasted Times to the real Times:
```{r}
df1 <- data.frame(data$Time, data$ForecastedTime, data$year)
df2 <- melt(df1, id.vars='data.year')

ggplot(df2, aes(x=data.year, y=value, fill=variable)) +
    geom_bar(stat='identity', position='dodge')
```

We can see how the Forecasted time is pretty on par with the real time,
so the prediction is very good and varies very little in small units.

The worst predictions have been obtained on the 1st and 3rd year. And it has nailed
the predictions on the 2nd, 5th and 6th year. An overestimation has been done on the 4th year.

```{r}
ggplot(data = data) +
  aes(x = year, y = Distance) +
  geom_boxplot(fill = "#4292c6") +
  theme_minimal()
```

In the plot above we can see how for each year, the distance does not vary a lot, except on the 3rd year, where there seems to be a higher overall distance but with small variance. The 1st year seems to be the one with most variance on distances between stages and the 5th one does not seem to vary so much, except on some stages qhere the distance differs more (between 100 and past 250). The 6th year seems to be the year with less variance in distance between stages.

Since the 4th year has very little distance, it may have affected the forecasted Time, which was above the real time.

```{r}
ggplot(data = data) +
  aes(x = year, y = AccumIncr) +
  geom_boxplot(fill = "#4292c6") +
  theme_minimal()
```
Now, taking a look at the accumulated number of meters climbed on the stages for each year, 
we can see that the 2nd year is the one with most variance and, in general, most of them
vary a lot on the upper bound but not so much on the lower one.

The 3rd year seems to be the one with a highest median and the 4th one seems to be the one with the lowest one.

Now we are going to use a boxplot to see where most ports are. For example, with portsE:
```{r}
ggplot(data = data) +
  aes(x = year, y = portsE) +
  geom_boxplot(scale = 'area', adjust = 1, fill = '#0c4c8a') +
  theme_minimal()
```
We can see that that boxplot is not hat helpful, since we explained that it only has integer variables.

Thus we are going to stick to the tables to see how the number of mountain passes distributes:
```{r}
table(data$portsE)
table(data$ports1)
table(data$ports2)
table(data$ports3)
```
Looking at the ports tables we can see that in portsE, there are mostly no mountain passes, twelve cases with 1 pass and 2 exceptions with 2 passes. On the ports1 and ports2 the same happens but there are some stages where there are 1 or 2 passes, with 3 exceptions with 3 passes. Finally, in ports3 the tendency of having no passes still lasts but there are more cases of 2 passes and two stages with 4 passes, which are the exceptions on the tendency.

## Correlation plot

In order to see if we can find some kind of relation between the variables of our dataset we will use a correlations plot:

```{r}
plot_correlation(data)
```

From this plot we immediately see that Time and Distance are highly correlated. This makes a lot of sense because, as a general rule, the longer the stage is the longer it will take cyclists to complete. Of course we could find stages that are long but downhill, which would make it possible for the cyclists to complete a long stage in less time, although this is not the usual. This tells us that probably Distance will be an important variable when it comes to predicting the value of the variable Time as a linear combination of the other variables.

It also seems apparent that Time and ForecastedTime are highly correlated, which was to be expected since these two variables represent the same data and ForecastedTime is a prediction of the variable Time. No other relevant correlations can be observed.

# Model selection

Let's first try creating a linear model. We believe that the year has no influence over the stages so we will not include it into our model.

```{r}
lm.model <- lm(Time ~ Distance + HeightIncr + AccumIncr  + ports1 + ports2 + ports3 + bef_mount + aft_mount + bef_tt + aft_tt + last, data)
summary(lm.model)
```
We can see we have a high adjusted R squared. However there are many variables that are not contributing to the the model, so we will take them away. The variables that we will keep are: Distance, heightIncr AccumIncr. We will also keep Ports2 even though it doesn't seem to be that important. 

```{r}
lm.model2 <- lm(Time ~ Distance  + AccumIncr + ports2, data)
summary(lm.model2)
```
With this new model we can see that the adjusted $R^2$ has increased because we used less parameters, so we will keep this model for now. However we could try removing ports2 since it has been deemed irrelevant.
```{r}
lm.model3 <- lm(Time ~ Distance  + AccumIncr, data)
summary(lm.model3)
```
Both $R^2$ and Adjusted $R^2$ have decreased ever so slightly but, considering we took one parameter away we will probably keep this model.

Performing a Chisquare test with anova, we can decide which model fits better:
```{r}
anova(lm.model2, lm.model, test="Chisq")
```
We can see that the p-value is greater than 0.05, so we are going to choose lm.model2 over lm.model, which has almost all variables.

Now, if we check whether to choose lm.model3 or lm.model2, we can see that again the p-value is greater than 0.05, so we are
going to choose the simpler model:  Time ~ Distance + AccumIncr.

```{r}
anova(lm.model3, lm.model2, test="Chisq")
```

Since these are nested models, we are going to check their deviance, AIC, perform again the Pearson test(Chi square):

```{r}

computeMetrics <- function(m, df) {
  
    cat("Deviance: ", deviance(m), "\n")
    cat("AIC: ", AIC(m), "\n")
    cat("Pearson test X^2: ", sum(residuals(m, type = "pearson")^2), "\n")  
    cat(" X^2/(N-P): ", sum(residuals(m, type = "pearson")^2)/(nrow(df) - length(m$coefficients)), "\n")
}
cat(" \n * Metrics for lm.model \n ------- \n")
computeMetrics(lm.model, data)
cat(" \n\n * Metrics for lm.model2 \n ------- \n")
computeMetrics(lm.model2, data)
cat("\n\n * Metrics for lm.model3 \n ------- \n")
computeMetrics(lm.model3, data)
```

We can see that the difference on the deviance on the 3 models is small. The lowest AIC is obtained with the third model, although only by a small margin. Performing the Chi square test we can see that the greatest value is found on the 3rd model, and is the same as the deviance, for we are working with a linear model, thus assuming a normal distribution.

So we have decided to work with the linear model number 3, which is the simplest one, lm.model3, that follows the formula $$Time \sim Distance + AccumIncr$$


## Residuals check.

In this section we will perform residual analysis in order to test whether the assumptions for our linear model are actually fulfilled.

```{r}
ncol(data)
nrow(data)
```
Since we have 105 observations and 16 parameters, thus $$ N_{obs} >> p$$
and we can check the histogram of standard residuals.
```{r}
hist(stdres(lm.model3), col="darkred")
```
It is easy to see that the shape of histogram of the residuals resemble a normal distribution. 

```{r}
plot(fitted(lm.model3), stdres(lm.model3))
abline(h=0, col="red")
```

We can see that there is no shape and the points are just scattered randomly, thus there is no pattern.

It seems that 95% or more of the points range between -2 and 2, and thus we can see that Homoscedasticity is complied because there is no shape.


If we qqplot below the residuals we can see that the standardised residuals follow a normal distirbution and thus the normality assumption is satisfied.

```{r}
res = stdres(lm.model3)
qqnorm(res); qqline(res)
```

Finally we are going to check the independence of resiudals against the predicted value:
```{r}
res <- residuals(lm.model3)
plot(res ~ data$Time, ylab = 'Residuals', xlab = 'Time', main = 'Residuals VS Time')
abline(h = 0, col='red')
```
We can see that there is indeed no pattern in the residuals vs time so we can assume independence of variables.

# Predictions

Now we are going to use our model to predict the time of each stage:
```{r}
timePredictions <- predict(lm.model3, data)
```

Let's compare the predictions with the real time:
```{r}
dfOrg <- data.frame(timePredictions, data$Time, data$year)
dfOrg2 <- melt(dfOrg, id.vars='data.year')

ggplot(dfOrg2, aes(x=data.year, y=value, fill=variable)) +
    geom_bar(stat='identity', position='dodge')
```
We can see that the predictions fall a little bit short  in the first 3 years, but are pretty accurate on the last 3 years.

Let's compare the Time variables:

- Time: The real time.
- Forecasted Time: The time prediction made by an expert.
- timePredictions: Our predictions using our model following the formula `Time ~ Distance + AccumIncr`.

```{r}
dfOrgExp <- data.frame(timePredictions, data$Time, data$ForecastedTime, data$year)
dfOrgExp2 <- melt(dfOrgExp, id.vars='data.year')

ggplot(dfOrgExp2, aes(x=data.year, y=value, fill=variable)) +
    geom_bar(stat='identity', position='dodge')
```

We can see how our predictions are pretty similar, almost identical, to those done by an expert.
The only notable difference is that our model falls shorter on the prediction in the second year. 

On the other hand, it does better in the 4th and 6th year, surpassing the real time way less than
the prediction made by an expert, with surpasses the predictions by more units.

In conclusion, looking at the graphic above, we are pretty satisfied with the model we have created and the predictions
are good enough.


# Generalized Linear Model

Having done a Linear Model, we have asked ourselves if there was any motivation to do a Generalized Linear Model.

As fas as we know, Time follows a Normal Distribution. This means that the response variable follows a Gaussian distribution, and thus the family of the GLM would be a Gaussian. Having as the link function the identity, then our Linear Model is a subcase of a Generalized Linear Model. Because of this, creating a GLM with a Gaussian and the identity as the link function implies having the same Linear Model that we have. 

Just to verify it, let's create a GLM with the same formula, stating that the family is Gaussian and that the link function is "identity".
```{r}
glm.model <-glm(Time ~ Distance + AccumIncr, data=data, family=gaussian(link="identity"))
timePredGaussian <- predict(glm.model , data)

dfOrgExp <- data.frame(timePredGaussian, timePredictions, data$Time, data$ForecastedTime, data$year)
dfOrgExp2 <- melt(dfOrgExp, id.vars='data.year')

ggplot(dfOrgExp2, aes(x=data.year, y=value, fill=variable)) +
    geom_bar(stat='identity', position='dodge')
```

We can see how, indeed, the predictions are the same because the model is the same. 
And if we take a look at the metrics, we can further state that, indeed, they are the same model:
```{r}
# m: model
# df: Data Frame
computeMetrics <- function(m, df) {
  
    cat("Deviance: ", deviance(m), "\n")
    cat("AIC: ", AIC(m), "\n")
    cat("Pearson test X^2: ", sum(residuals(m, type = "pearson")^2), "\n")  
    cat(" X^2/(N-P): ", sum(residuals(m, type = "pearson")^2)/(nrow(df) - length(m$coefficients)), "\n")
}
computeMetrics(lm.model3, data)
computeMetrics(glm.model, data)
```

## Formula change

So, instead of using the previous formula chosen by us looking at the R squared and using the Pearson Test,
we are going to select the terms by using the AIC on the model.

So we start with the whole terms that we deem important, leaving out the years, for they should not determine the time and are only informative:

```{r}
glm.model <-glm(Time ~ Distance + HeightIncr + AccumIncr  + ports1 + ports2 + ports3 + bef_mount + aft_mount + bef_tt + aft_tt + last, data=data, family=gaussian(link="identity"))
```

```{r results='hide'}
glm.model = stepAIC(glm.model)
```

```{r}

summary(glm.model)
timePredGaussian <- predict(glm.model , data)

dfOrgExp <- data.frame(timePredGaussian, timePredictions, data$Time, data$ForecastedTime, data$year)
dfOrgExp2 <- melt(dfOrgExp, id.vars='data.year')

ggplot(dfOrgExp2, aes(x=data.year, y=value, fill=variable)) +
    geom_bar(stat='identity', position='dodge')
```

We can see that the final formula is: $$Time \sim Distance + AccumIncr + ports1 + ports2$$
so now it takes into account ports1 and ports2.

The predictions have improved slightly on the 1st and 3rd but are not that well on the 2nd, 4th and 6th.
So for now we prefer the linear model over this one.

```{r}
anova(lm.model3, glm.model, test='Chisq')
```

Additionally, using the Chi square test we can see that the residual sum of squares is greater on the first model, which is to be expected for it is a simpler model. However, looking at the p-value we can see that it is greater than 0.05, thus we are sticking with the simpler linear model.


## GLM: Gaussian distribution, link function: log
Now, since we know that the Time follows a Normal distribution, we will keep the family as a Gaussian, but we will use
as link function the logarithm. This means that once we have predicted the values we have to use an exponential to change
them to the original units:


```{r}
glm.model2 <-glm(Time  ~ Distance + HeightIncr + AccumIncr  + ports1 + ports2 + ports3 + bef_mount + aft_mount + bef_tt + aft_tt + last, data=data, family=gaussian(link = "log"))
```

```{r results='hide'}
glm.model2 = stepAIC(glm.model2)
```
```{r}
summary(glm.model2)
```

We can see how the formula now takes into account ports1, 2 and 3, as well as the distance and AccumIncr as before.

```{r}
timePredGaussianLog <- predict(glm.model2 , data)
timePredGaussianLog <- exp(timePredGaussianLog)

dfOrgExp <- data.frame(timePredGaussianLog, timePredictions, data$Time, data$ForecastedTime, data$year)
dfOrgExp2 <- melt(dfOrgExp, id.vars='data.year')

ggplot(dfOrgExp2, aes(x=data.year, y=value, fill=variable)) +
    geom_bar(stat='identity', position='dodge')
```

We can see how this model seems to get closer to the real values in the first 3 years. However, we can see how it surpasses 
by a great amount the prediction in the 5th year.

```{r}
anova(lm.model3, glm.model2, test="Chisq")
```

Performing an anova with Chisquare test we can see how again we are going to stick with the Linear Model.

## GLM: Gaussian distribution, link function: inverse

Let's check if using the inverse function serves any purpose or better results:

```{r}
glm.model3 <-glm(Time  ~ Distance + HeightIncr + AccumIncr  + ports1 + ports2 + ports3 + bef_mount + aft_mount + bef_tt + aft_tt + last, data=data, family=gaussian(link = "inverse"))
```

```{r results='hide'}
glm.model3 = stepAIC(glm.model3)
```
```{r}
summary(glm.model3)
```

We can see how the formula now takes into bef_mount, as well as the distance and AccumIncr as before. Ports have been totally dismissed.


Using the inverse as the link function does not help either:
```{r}
timePredGaussianInv <- predict(glm.model3 , data)
timePredGaussianInv <- 1/timePredGaussianInv

dfOrgExp <- data.frame(timePredGaussianInv, timePredictions, data$Time, data$ForecastedTime, data$year)
dfOrgExp2 <- melt(dfOrgExp, id.vars='data.year')

ggplot(dfOrgExp2, aes(x=data.year, y=value, fill=variable)) +
    geom_bar(stat='identity', position='dodge')
```

The predictions are pretty far off on the 5th year but better on the rest of years.

```{r}
anova(lm.model3, glm.model3, test="Chisq")
AIC(lm.model3)
AIC(glm.model3)
```
We can see that the AIC are pretty similar, but it is smaller in the linear model. So we are going to stick to that one.

## GLM: Gamma distribution, link function: identity

Since time cannot be negative and is a continuous variable, we are going to consider using
the Gamma distribution as the family distribution for the response variable.

Let's first start with the identity as the link function:


```{r}
glm.model4 <-glm(Time ~ Distance + AccumIncr, data=data, family=Gamma(link = "identity"))
timePredGammaId <- predict(glm.model4 , data)

dfOrgExp <- data.frame(timePredGammaId, timePredictions, data$Time, data$ForecastedTime, data$year)
dfOrgExp2 <- melt(dfOrgExp, id.vars='data.year')

ggplot(dfOrgExp2, aes(x=data.year, y=value, fill=variable)) +
    geom_bar(stat='identity', position='dodge')
```
We can see how there's not much of an improvement using the identity as link function.
Let's try using the logarithm:

```{r}
glm.model5 <-glm(Time~Distance + AccumIncr, data=data, family=Gamma(link="log"))
timePredGammaLog <- predict(glm.model5 , data)
timePredGammaLog <- exp(timePredGammaLog)

dfOrgExp <- data.frame(timePredGammaLog, timePredictions, data$Time, data$ForecastedTime, data$year)
dfOrgExp2 <- melt(dfOrgExp, id.vars='data.year')

ggplot(dfOrgExp2, aes(x=data.year, y=value, fill=variable)) +
    geom_bar(stat='identity', position='dodge')
```
We can, again, see how there is an improvement over the linear model in the first 3 years, but a 
greater amount on the prediction in the 4th and 5th years.

## All models comparison
```{r}

dfOrgExp <- data.frame(timePredGaussian, timePredGaussianInv, timePredGaussianLog, timePredGammaId, timePredGammaLog, timePredictions, data$Time, data$year)
dfOrgExp2 <- melt(dfOrgExp, id.vars='data.year')

ggplot(dfOrgExp2, aes(x=data.year, y=value, fill=variable)) +
    geom_bar(stat='identity', position='dodge')
```


Looking at the graphic above, we can see that all predictions are short on the first year.
The Gaussian with inverse link function seems to be the one that performs better on the second year. However,
it is also the worst performing on the 4th and 5th year, overestimating the time a lot.

By looking at these comparisons, we prefer to use the linear model over any Generalized Linear model.


# Conclusions

After many tests and having checked the response variable, we can determine that:

1. Time follows a Normal Distribution.
2. Hypothesis of homocedasticity, normality and independence cannot be rejected.
3. A Linear Model using Distance and AccumIncr seems to perform well with the data at hand and the predictions are quite accurate.
4. We cannot justify the usage of a Generalized Linear Model. 

## Future Estimations

This model has been created using observations from 6 years. We think that with the data at hand, we would be able to estimate accurately enough the Time for the riders to finish each stage, as long as the organizers of `La Vuelta` do not change a lot other parameters than Distance and AccumIncr that may have a greater impact than now.

For example, we can take a strange case where there are a lot of ports (mountain passes). We would not be taking them into account
in our model. However, having a lot of ports may have an impact on the number of stops done by a rider, thus having an impact on the final time.

However, organizers of `La Vuelta` tend and try to make the competition similar to other years so that it is fair
for those competitors participating each year, so that having won on one year holds the same value as having won 3 years later.

Thus, we think our model will hold well enough for another 15 `Vueltas` later, although having more data for more years
would have been better, since we are trying to predict the Times for twice the years of data we have currently.