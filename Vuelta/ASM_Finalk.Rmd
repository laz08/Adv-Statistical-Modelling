---
title: "La Vuelta"
author: "Laura Cebollero Ruiz, Alexandre Rodr√≠guez Garau"
date: "4th January, 2019"
output: pdf_document
---

# Introduction
In this project we intend to predict the duration of the different stages of the cycling race *La Vuelta* using the information provided in the file $\texttt{Vuelta0.mtp}$. 


```{r}
library("readxl")
library("knitr")
library("ggplot2")
library("reshape2")
library("MASS")
library("DataExplorer")
```

We have exported the data to an extension that R can read: `.csv`.

```{r}
data<- read.csv("Vuelta01.csv", sep = ';', header = TRUE, dec=",")
```


# Data exploration

To predict the length of the stages we will use a set that contains 14 explanatory variables and a response variable called __`ForecastedTime`__. Let's take a look at the summary of the variables:

```{r}
kable(summary(data[,1:6]))
kable(summary(data[,6:11]))
kable(summary(data[,12:16]))
```

In the previous tables we can see a short summary of the variables. __`ports1`__, __`ports2`__ and __`ports3`__ indicate the number of mountain sections in a given stage and their category (1,2 or 3). __`year`__ corresponds to the year in which this data was recorded (1 to 6). __`Week`__ is the week of the race in wich the stage takes place (1 to 3). The variables __`bef_mount`__ and __`aft_mount`__ tell us whether a stage took place before or after a mountain stage, respectively. Similarly, the variables __`bef_tt`__ and __`aft_tt`__ indicate if a stage took place before or after a time trial stage. Finally, the variable __`last`__ tells us if that stage was the last of the whole race. 

By the looks of the summary we can't seem to find any outliers or abnormal values. Most of the explanatory variables range from 0 to very low values and are natural numbers. The only continuous variables are __`ForecastedTime`__, __`Distance`__, __`HeightInc`__ and __`AccumIncr`__. This makes sense because the first variable indicates time and time is a continuous variable and the rest indicate distance which can also be continous. 

The variable __`Distance`__ seems to be quite balanced with a mean of 193 and min and max values of 111 and 264 respectively. __`HeightIncr`__, however, is the only variable that presents negative values and has very high variance. Its minimum value is -940 and the maximum value is 2310. 

By taking a closer look at the data we detect some abnormal values in the __`last`__ variable: Since this variable indicates if a stage was the last of the whole race, then there should as many stages with this value equal to 1 as years of data have been recorded. Since the recorded stages are from 6 different years then there should be 6 rows, but instead there are only 5, meaning that there is at least 1 missing stage. 

```{r}
data[data$last == 1,]$year
```

If we look at the data we can easily see that the missing value belongs to the year 3. However, this will probably not greatly affect our predicting power. There is something to be said about the variables __`year`__, __`week`__, __`bef_mount`__, __`aft_mount`__, __`bef_tt`__, __`aft_tt`__ and __`last`__: all of these variables are categorical and indicate the group or category a row belongs to. For this, we should transform these variables into factors.

```{r}
data$year      <- as.factor(data$year)
data$week      <- as.factor(data$week)
data$bef_mount <- as.factor(data$bef_mount)
data$aft_mount <- as.factor(data$aft_mount)
data$aft_tt    <- as.factor(data$aft_tt)
data$bef_tt    <- as.factor(data$bef_tt)
data$last      <- as.factor(data$last) 
```
\newpage
Now that we have transformed the categorical columns to factors we should take another look at the summary of these variables:

```{r}
kable(summary(data[,9:15]))
```

We can se that for the variables __`year`__ and __`week`__ the variables are quite balanced, each group contains a very similar amount of observations. The rest, however, are more unbalanced. `bef_mount` has more than twice 0s than 1s, which means there are twice the stages that do not precede a mountain stage. The same happens with `aft_mount`, which makes sense for it means there are only 31 stages after a mountain stage. Since they are the same numbers as the `bef_mount`, this tells us that the mountain stages are always in the middle and are not the first or last stages.

In `bef_tt` there are 9 times more stages **not** preceding a time trial stage than does preceding it. There are 4 times less after trial stages.  And finally, as mentioned before, there are only 5 last stages and 100 that are normal ones.

Let's finally take a closer look to the numerical attributes:
```{r}
num <- 6:8; num <- append(num, 16)
kable(summary(data[,1:5]))
kable(summary(data[,num]))
```

We can see that the mean and median in Time are pretty close, which means that there is not an outlier that drastically decreases or increases the mean. The same happens with Distance, AccumIncr and ForacastedTime. 

However, HeightIncr has a low Median but a high mean, which means that in general there is not a stable increase of height but some stages have a high increase of it. And we can see that there is at least one stage that consists of a decrease of almost 1km, whereas there is at least one stage of an increase of over 2km.

As for the ports, although it is a numerical variable, it actually uses integer values and natural ones should not allowed.
We contemplated using the number of ports as a categorical variable. For example, in portsE we can see there are between 0 and 2 mountain passes:
```{r}
table(data$portsE)
```

However, by enforcing said restriction changing the variable as a categorical one, we would not be allowing new categorical variables like using 5 mountain passes in stage port3. Thus, meaning we would be restricted to a maximum of 3 in ports1 and ports2, 2 mountain passes on portsE, and a maximum of 4 mountain passes in ports3, which would not be correct, for maybe in the future they may be a case where there are 3 mountain passes in portsE, where there is still no case as seen in the table above.


# Data exploration visualization

Now, in order to find outliers and to see how the data behaves we will plot some boxplots. First of all we would like to see if the forecasted times are different depending on the year in which the race took place. 

```{r}
ggplot(data = data) +
  aes(x = year, y = ForecastedTime) +
  geom_boxplot(fill = "#4292c6") +
  theme_minimal()
```

At first glance we see that the medians are really similar among all years, excepting year 3 and year 6, which are about 25 units higher. The interquartile ranges for every year are different. Even though the Q3 percentile is very similar for all years, the Q1 percentile varies from 225 to 300. It is important to say that there appears to be an outlier in the second year.

Let's take a look at real Times:
```{r}
ggplot(data = data) +
  aes(x = year, y = Time) +
  geom_boxplot(fill = "#4292c6") +
  theme_minimal()
```
 We can see how the highest Time is on the 3rd year, and there's a lot of variance on the
 1st and 5th years' races.
 The second year seems to have some stages with a very high time, so there's a lot of variance on the upper bound,
while on the 5th year there does is a lot of variance on the lower bound, meaning there are some stages with less Time than the rest. 
The 5th year is the only one where this phenomena is so visible.

 And let's compare the forecasted Times to the real Times:
```{r}
df1 <- data.frame(data$Time, data$ForecastedTime, data$year)
df2 <- melt(df1, id.vars='data.year')

ggplot(df2, aes(x=data.year, y=value, fill=variable)) +
    geom_bar(stat='identity', position='dodge')
```

We can see how the Forecasted time is pretty on par with the real time,
so the prediction is very good and varies very little in small units.

The worst predictions have been obtained on the 1st and 3rd year. And it has nailed
the predictions on the 2nd, 5th and 6th year. An overestimation has been done on the 4th year.

```{r}
ggplot(data = data) +
  aes(x = year, y = Distance) +
  geom_boxplot(fill = "#4292c6") +
  theme_minimal()
```

In the plot above we can see how for each year, the distance does not vary a lot, except on the 3rd year, where there seems to be a higher overall distance but with small variance. The 1st year seems to be the one with most variance on distances between stages and the 5th one does not seem to vary so much, except on some stages qhere the distance differs more (between 100 and past 250). The 6th year seems to be the year with less variance in distance between stages.

Since the 4th year has very little distance, it may have affected the forecasted Time, which was above the real time.

```{r}
ggplot(data = data) +
  aes(x = year, y = AccumIncr) +
  geom_boxplot(fill = "#4292c6") +
  theme_minimal()
```
Now, taking a look at the accumulated number of meters climbed on the stages for each year, 
we can see that the 2nd year is the one with most variance and, in general, most of them
vary a lot on the upper bound but not so much on the lower one.

The 3rd year seems to be the one with a highest median and the 4th one seems to be the one with the lowest one.

Now we are going to use a boxplot to see where most ports are. For example, with portsE:
```{r}
ggplot(data = data) +
  aes(x = year, y = portsE) +
  geom_boxplot(scale = 'area', adjust = 1, fill = '#0c4c8a') +
  theme_minimal()
```
We can see that that boxplot is not hat helpful, since we explained that it only has integer variables.

Thus we are going to stick to the tables to see how the number of mountain passes distributes:
```{r}
table(data$portsE)
table(data$ports1)
table(data$ports2)
table(data$ports3)
```
Looking at the ports tables we can see that in portsE, there are mostly no mountain passes, twelve cases with 1 pass and 2 exceptions with 2 passes. On the ports1 and ports2 the same happens but there are some stages where there are 1 or 2 passes, with 3 exceptions with 3 passes. Finally, in ports3 the tendency of having no passes still lasts but there are more cases of 2 passes and two stages with 4 passes, which are the exceptions on the tendency.

## Correlation plot

In order to see if we can find some kind of relation between the variables of our dataset we will use a correlations plot:

```{r}
plot_correlation(data)
```

From this plot we immediately see that Time and Distance are highly correlated. This makes a lot of sense because, as a general rule, the longer the stage is the longer it will take cyclists to complete. Of course we could find stages that are long but downhill, which would make it possible for the cyclists to complete a long stage in less time, although this is not the usual. This tells us that probably Distance will be an important variable when it comes to predicting the value of the variable Time as a linear combination of the other variables.

It also seems apparent that Time and ForecastedTime are highly correlated, which was to be expected since these two variables represent the same data and ForecastedTime is a prediction of the variable Time. No other relevant correlations can be observed.

# Model selection

Let's first try creating a linear model. We believe that the year has no influence over the stages so we will not include it into our model.

```{r}
lm.model <- lm(Time ~ Distance + HeightIncr + AccumIncr  + ports1 + ports2 + ports3 + bef_mount + aft_mount + bef_tt + aft_tt + last, data)
summary(lm.model)
```
We can see we have a high adjusted R squared. However there are many varaibles that are not contributing to the the model, so we will take them away. The variables that we will keep are: Distance, heightIncr AccumIncr. We will also keep Ports3 even though it doesn't seem to be that important. 

```{r}
lm.model2 <- lm(Time ~ Distance  + AccumIncr + ports2, data)
summary(lm.model2)
```
With this new model we can see that the adjusted $R^2$ has increased because we used less parameters, so we will keep this model for now. However we could try removing ports2 since it has been deemed irrelevant.
```{r}
lm.model3 <- lm(Time ~ Distance  + AccumIncr, data)
summary(lm.model3)
```
Both $R^2$ and Adjusted $R^2$ have decreased ever so slightly but, considering we took one parameter away we will keep this model.

## Residuals check.

In this section we will perform residual analysis in order to test whether the assumptions for our linear model are actually fulfilled.

```{r}
ncol(data)
nrow(data)
```
Since we have 105 observations and 16 parameters, thus $$ N_{obs} >> p$$
and we can check the histogram of standard residuals.
```{r}
hist(stdres(lm.model3), col="darkred")
```
It is easy to see that the shape of histogram of the residuals resemble a normal distribution. 

```{r}
plot(fitted(lm.model3), stdres(lm.model3))
abline(h=0, col="red")
```

We can see that there is no shape and the points are just scattered randomly, thus there is no pattern.

It seems that 95% or more of the points range between -2 and 2, and thus we can see that Homoscedasticity is complied because there is no shape.


If we qqplot below the residuals we can see that the standardised residuals follow a normal distirbution and thus the normality assumption is satisfied.

```{r}
res = stdres(lm.model3)
qqnorm(res); qqline(res)
```

Finally we are going to check the independence of resiudals against the predicted value:
```{r}
res <- residuals(lm.model3)
plot(res ~ data$Time, ylab = 'Residuals', xlab = 'Time', main = 'Residuals VS Time')
abline(h = 0, col='red')
```
We can see that there is indeed no pattern in the residuals vs time so we can assume independence of variables.

# Generalized Linear Model

Now we are going to try to model a Generalized Linear Model for our data in order to
predict the Time.

We assume Time follows a general probability distribution (not necessarily a Normal distribution),
and we are going to assume there is some linearity between the transformation of Time $\mu$ where
$$\mu = E(Time)$$
 and the covariates.
 
# Prediction

In this section we are going to compare our predictions with those on the Forecasted by an expert.

