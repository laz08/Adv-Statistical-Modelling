---
title: "Lab2"
author: "Sergi Carol Laura Cebollero Alex Rodriguez"
date: "November 15, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(glmnet)
data(Boston) # TODO: Check for the Boston Corrected dataset (where to download it?)
source("all_functions.R")
```


```{r}
set.seed(123)

lambda.max <- 1e5
n.lambdas <- 74
lambda.v <- exp(seq(0,log(lambda.max+1),length=n.lambdas))-1
## Data exploration
summary(Boston)

Y <- scale( Boston$medv, center=TRUE, scale=FALSE)
X <- scale( as.matrix(Boston[,1:13]), center=TRUE, scale=TRUE)
n <- dim(X)[1]
p <- dim(X)[2]
XtX <- t(X)%*%X 
d2 <- eigen(XtX,symmetric = TRUE, only.values = TRUE)$values
df.v <- numeric(n.lambdas)
for (l in 1:n.lambdas){
  lambda <- lambda.v[l]
  df.v[l] <- sum(d2/(d2+lambda)) 
}

```

### 1  For the Boston House-price corrected dataset use Lasso estimation (in glmnet) to fit the regression model where the response is CMEDV (the corrected version of MEDV) and the explanatory variables are the remaining 13 variables in the previous list. Try to provide an interpretation to the estimated model.

We want to note that we did not find the mentioned dataset in the raco, and as such we have used the one in the package _MASS_, which does not have the corrected version of MEDV, and as such we will be using MEDV as our target variable.

```{r}
m1.lasso <- glmnet(X, Y, lambda= lambda.v)
summary(m1.lasso)
plot(m1.lasso, xvar="lambda")
plot(log(m1.lasso$lambda), m1.lasso$dev.ratio, type='b')
```

Caldria computar el lambda minim + sd per saber quin valor agafar


### 2 Use glmnet to fit the previous model using ridge regression. Compare the 10-fold cross validation results from function cv.glmnet with those you obtained in the previous practice with your own functions.

```{r}
m2.rr <- glmnet(X, Y, alpha=0)
summary(m2.rr)
plot(m2.rr,  xvar="lambda")
plot(log(m2.rr$lambda), m2.rr$dev.ratio, type='b')
# Same as up
```
As expected we can see how the error goes down the higer the value of the lamdas.
```{r}
m3.rr <- cv.glmnet(X, Y, nfolds=10, lambda =  lambda.v, standardize=FALSE, intercept=FALSE, alpha=0)
plot(m3.rr)
abline(v=log(m3.rr$lambda.min),col=2,lty=2)
abline(v=log(m3.rr$lambda.1se),col=2,lty=2)
cvm <- rev(m3.rr$cvm)

m4.rr <-  MSPEkfold(X, Y, 10, n.lambdas, lambda.v, n)
points(log(lambda.v), m4.rr, col=5)

min.lambda <- data.frame("K.fold" = lambda.v[which.min(m4.rr)], "glmnet" = m3.rr$lambda.min)
min.df <- data.frame("K.fold" = df.v[which.min(m4.rr)],"glmnet" =df.v[which(lambda.v == m3.rr$lambda.min)])
(result <- rbind(min.df, min.lambda))
```


```{r}
lambda.glm <- m3.rr$lambda
df.v.g <- numeric(n.lambdas)
for (l in 1:n.lambdas){
  lambda <- lambda.glm[l]
  df.v.g[l] <- sum(d2/(d2+lambda)) 
}
plot(df.v, m3.rr$cvm, col=1,pch=19,cex=.75)
points(df.v, m4.rr,col=8,pch=19,cex=.75)
```


