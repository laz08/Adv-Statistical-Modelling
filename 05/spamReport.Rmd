---
title: "ASM - LASSO Spam"
author: "Sergi Carol, Laura Cebollero, Alex Rodriguez"
date: "21st November, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
library("data.table")
library("knitr")
library("glmnet")
library("caret")
library("ROCR")    
```

# Introduction



# Reading the data
```{r}
source("spam.R")
```


# Preparing the data

```{r}
mail.spam = spam[which(spam$spam.01 == 1), ]
mail.non.spam = spam[which(spam$spam.01 == 0), ]


## Proportions of Spam
train.prop.spam = 2*nrow(mail.spam)/3

spam.training = mail.spam[1:train.prop.spam,]
spam.val = mail.spam[train.prop.spam:nrow(mail.spam),]


## Proportions non-spam
train.prop.no.spam = 2*nrow(mail.non.spam)/3

non.spam.training = mail.non.spam[1:train.prop.no.spam,]
non.spam.val = mail.non.spam[train.prop.no.spam:nrow(mail.non.spam),]

## Merging

train.set = rbind(spam.training, non.spam.training)
val.set = rbind(spam.val, non.spam.val)
train.set$spam.01 <-  as.factor(train.set$spam.01)
val.set$spam.01 <-  as.factor(val.set$spam.01)

## Scaling and centering

x.train <- as.matrix(train.set[, 1:57])
y.train <- as.factor(train.set$spam.01)

# Scale validation set
x.val <- as.matrix(val.set[, 1:57])
y.val <- as.factor(val.set$spam.01)

table(y.val)

```

# Classification rules

## Logistic regression fitted by maximum likelihood.

```{r}

m1 <- glm(spam.01 ~ .,data=train.set, family=binomial)
## you don't need to worry about this warning.  
## It says that some covariates are nearly perfect predictors.
plot(m1$fit~train.set$spam.01, 
     xlab="", ylab=c("fitted probability of spam"), 
     col=c("navy","red"))
y.pred <- predict(m1, as.data.frame(x.val), type="response")
summary(y.pred)

y_pred_num <- ifelse(y.pred < 0.5, 0, 1)
table(y_pred_num)

cm = confusionMatrix(data = as.factor(y_pred_num), as.factor(val.set$spam.01))
cm$table
cm$byClass
cm$overall

#auc(val.set$spam.01, y.pred)

pred <- prediction(y.pred, val.set$spam.01)    
perf <- performance(pred, measure = "tpr", x.measure = "fpr")     
plot(perf, col=rainbow(7), main="ROC curve", xlab="Specificity", 
     ylab="Sensitivity")    
abline(0, 1) #add a 45 degree line
```


## Logistic regression fitted by LASSO

```{r}
m2 <- cv.glmnet(x.train, y.train, alpha=1, family = "binomial", standardize=FALSE, nfolds=10)
plot(m2)
#m2 <- glmnet(x.train, y.train, alpha=1, family = "binomial", standardize=FALSE)
y.pred <- predict(m2, x.val, type="response")
y_pred_num <- ifelse(y.pred < 0.5, 0, 1)

cm = confusionMatrix(data = as.factor(y_pred_num), as.factor(val.set$spam.01))
cm$table
cm$byClass
cm$overall
```

## K-NN binary regression


<!-- For each rule, state the missclassifcation rate and so on.

Optional:  Compute the confussion table + ROC + AUC -->

